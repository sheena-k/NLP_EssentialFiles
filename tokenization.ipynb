{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0100b20c-52d7-4caa-a0b0-323b403a8dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\"\" Welcome to NLP basics! This notebook file is an example of tokenization using NLTK. NLTookKit(NLTK) and Spacy are two important libraries in NLP. NLTK is widely used for research and educational purpose and SpaCy is used in production ready environment. Let's tokenize corpus.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "0870084d-48b7-4b46-8534-4e637514d3b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Welcome to NLP basics! This notebook file is an example of tokenization using NLTK. NLTookKit(NLTK) and Spacy are two important libraries in NLP. NLTK is widely used for research and educational purpose and SpaCy is used in production ready environment. Let's tokenize corpus.\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "646eefc2-b880-4528-9ccf-f8eeb1fea7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Tokenization\n",
    "##Paragraph-->Sentences\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "73bea895-1400-4a06-a293-5fa29898167d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Welcome to NLP basics!',\n",
       " 'This notebook file is an example of tokenization using NLTK.',\n",
       " 'NLTookKit(NLTK) and Spacy are two important libraries in NLP.',\n",
       " 'NLTK is widely used for research and educational purpose and SpaCy is used in production ready environment.',\n",
       " \"Let's tokenize corpus.\"]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(corpus)  ### converting paragraphs to sentences as elements in of list. \n",
    "                        ## sent_tokenize(text, language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "a531b3d6-1d28-47a9-9bfc-d17e211de3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "document=sent_tokenize(corpus,language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1ab96f1c-f034-44d4-b3e8-a1fd572c8195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Welcome to NLP basics!\n",
      "This notebook file is an example of tokenization using NLTK.\n",
      "NLTookKit(NLTK) and Spacy are two important libraries in NLP.\n",
      "NLTK is widely used for research and educational purpose and SpaCy is used in production ready environment.\n",
      "Let's tokenize corpus.\n"
     ]
    }
   ],
   "source": [
    "for sentence in document:\n",
    "    print(sentence)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dd422c9f-0fb0-4f19-8424-75069bbd6431",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## Sentence-->words\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "949a7b12-0d8b-49f8-b06b-cfa1c2d8eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=wordpunct_tokenize(corpus) ## consider 's and other punct as separate word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "588686f9-d4b2-4d3f-9e32-debeb1198dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome\n",
      "to\n",
      "NLP\n",
      "basics\n",
      "!\n",
      "This\n",
      "notebook\n",
      "file\n",
      "is\n",
      "an\n",
      "example\n",
      "of\n",
      "tokenization\n",
      "using\n",
      "NLTK\n",
      ".\n",
      "NLTookKit\n",
      "(\n",
      "NLTK\n",
      ")\n",
      "and\n",
      "Spacy\n",
      "are\n",
      "two\n",
      "important\n",
      "libraries\n",
      "in\n",
      "NLP\n",
      ".\n",
      "NLTK\n",
      "is\n",
      "widely\n",
      "used\n",
      "for\n",
      "research\n",
      "and\n",
      "educational\n",
      "purpose\n",
      "and\n",
      "SpaCy\n",
      "is\n",
      "used\n",
      "in\n",
      "production\n",
      "ready\n",
      "environment\n",
      ".\n",
      "Let\n",
      "'\n",
      "s\n",
      "tokenize\n",
      "corpus\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "db712b95-bff9-4133-a999-889698466f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## Sentence-->words\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "c762a5e9-002d-438c-b4e8-e5fc662efbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "e21964bc-c48d-4f03-bf46-2b2f24491073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome\n",
      "to\n",
      "NLP\n",
      "basics\n",
      "!\n",
      "This\n",
      "notebook\n",
      "file\n",
      "is\n",
      "an\n",
      "example\n",
      "of\n",
      "tokenization\n",
      "using\n",
      "NLTK\n",
      ".\n",
      "NLTookKit\n",
      "(\n",
      "NLTK\n",
      ")\n",
      "and\n",
      "Spacy\n",
      "are\n",
      "two\n",
      "important\n",
      "libraries\n",
      "in\n",
      "NLP\n",
      ".\n",
      "NLTK\n",
      "is\n",
      "widely\n",
      "used\n",
      "for\n",
      "research\n",
      "and\n",
      "educational\n",
      "purpose\n",
      "and\n",
      "SpaCy\n",
      "is\n",
      "used\n",
      "in\n",
      "production\n",
      "ready\n",
      "environment\n",
      ".\n",
      "Let\n",
      "'s\n",
      "tokenize\n",
      "corpus\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for i in words:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "6cd7a152-f1ca-4b8c-bc43-6acc68c2ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "## Sentence-->words\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "cc87d45a-be80-470d-95f3-5cc38fb9bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=TreebankWordTokenizer()  ### \" . \" is considered and included with word but last word it will be seperated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c0bcd419-4e39-4e70-b4ab-5a4fbbdd8291",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Welcome',\n",
       " 'to',\n",
       " 'NLP',\n",
       " 'basics',\n",
       " '!',\n",
       " 'This',\n",
       " 'notebook',\n",
       " 'file',\n",
       " 'is',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'tokenization',\n",
       " 'using',\n",
       " 'NLTK.',\n",
       " 'NLTookKit',\n",
       " '(',\n",
       " 'NLTK',\n",
       " ')',\n",
       " 'and',\n",
       " 'Spacy',\n",
       " 'are',\n",
       " 'two',\n",
       " 'important',\n",
       " 'libraries',\n",
       " 'in',\n",
       " 'NLP.',\n",
       " 'NLTK',\n",
       " 'is',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'for',\n",
       " 'research',\n",
       " 'and',\n",
       " 'educational',\n",
       " 'purpose',\n",
       " 'and',\n",
       " 'SpaCy',\n",
       " 'is',\n",
       " 'used',\n",
       " 'in',\n",
       " 'production',\n",
       " 'ready',\n",
       " 'environment.',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " 'tokenize',\n",
       " 'corpus',\n",
       " '.']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7009abe-8e0f-4b60-946c-db29e18f9865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39201749-3c2c-497b-b033-89f99532b83b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d13995-2111-449b-90e6-722ecb69247e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
